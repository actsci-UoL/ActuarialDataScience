{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "NLP_Pipeline_Example.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciNJnSayVacI",
        "colab_type": "text"
      },
      "source": [
        "# *The Art of Natural Language Processing: NLP Pipeline*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fef_f6SVacP",
        "colab_type": "text"
      },
      "source": [
        "### **Authors: Andrea Ferrario, Mara Nägelin**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E3aYrSHVacW",
        "colab_type": "text"
      },
      "source": [
        "**Date: February 2020**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY10Eg85Vaca",
        "colab_type": "text"
      },
      "source": [
        "Notebook to test NLP preprocessing pipelines, as described in the tutorial `The Art of Natural Language Processing: Classical, Modern and Contemporary Approaches to Text Document Classification'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcnZroGvVace",
        "colab_type": "text"
      },
      "source": [
        "# Table of contents\n",
        "1. [Getting started with Python and Jupyter Notebook](#started)\n",
        "2. [Test sentence](#test)\n",
        "3. [NLP preprocessing pipelines](#pipeline)  \n",
        "    3.1. [Conversion of text to lowercase](#lower)  \n",
        "    3.2. [Tokenizers](#tokenizers)  \n",
        "    3.3. [Stopwords removal](#stopwords)  \n",
        "    3.4. [Part-of-speech tagging](#POS)  \n",
        "    3.5. [Stemming and lemmatization](#stemming)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTaDYrwcVaci",
        "colab_type": "text"
      },
      "source": [
        "# 1. Getting started with Python and Jupyter Notebook<a name=\"started\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emre4nlLVacl",
        "colab_type": "text"
      },
      "source": [
        "In this section, Jupyter Notebook and Python settings are initialized. For code in Python, the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) (\"PEP = Python Enhancement Proposal\") is enforced with minor variations to improve readibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9HrlXy_Vacv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "af509677-f3e2-4d44-d2a2-8d0e8fde2e2e"
      },
      "source": [
        "# Notebook settings\n",
        "###################\n",
        "\n",
        "# resetting variables\n",
        "get_ipython().magic('reset -sf') \n",
        "\n",
        "# formatting: cell width\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "\n",
        "# plotting\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDPQzHrLVadB",
        "colab_type": "text"
      },
      "source": [
        "# 2. Test sentence<a name=\"test\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuk9bTmOVadE",
        "colab_type": "text"
      },
      "source": [
        "We introduce the test sentence to be preprocessed with NLP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIIUttzqVadI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1291dc0c-6956-4250-ab9c-546b84fa8eb6"
      },
      "source": [
        "text = \"In H.P. Lovecraft's short story 'The Call of Cthulhu', the author states that in S. Latitude 47° 9', W. Longitude 126° 43' the great Cthulhu dreams in the sea-bottom city of R'lyeh.\"\n",
        "print(text)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In H.P. Lovecraft's short story 'The Call of Cthulhu', the author states that in S. Latitude 47° 9', W. Longitude 126° 43' the great Cthulhu dreams in the sea-bottom city of R'lyeh.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU0dW0QeVadX",
        "colab_type": "text"
      },
      "source": [
        "We follow the NLP pipeline:\n",
        "- conversion of text to lowercase;\n",
        "- tokenization, i.e. split of all strings of text into tokens;\n",
        "- part-of-speech (POS) tagging of tokenized text;\n",
        "- stopwords removal;\n",
        "- stemming or lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzpXe5rEVada",
        "colab_type": "text"
      },
      "source": [
        "# 3. NLP Preprocessing Pipeline<a name=\"pipeline\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJb9KxTtVadd",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Conversion of text to lowercase<a name=\"lower\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV0A88qhVadg",
        "colab_type": "text"
      },
      "source": [
        "We apply lowercase to the test sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "569Gl2deVadi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cfa6a3c1-5777-43bf-b9d5-d81720a40e66"
      },
      "source": [
        "text.lower()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"in h.p. lovecraft's short story 'the call of cthulhu', the author states that in s. latitude 47° 9', w. longitude 126° 43' the great cthulhu dreams in the sea-bottom city of r'lyeh.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SouwkEgVadx",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Tokenizers<a name=\"tokenizers\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taP_RFsSVadz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "45a2c564-b86d-46d1-cb9e-13d88e8a72f7"
      },
      "source": [
        "# 1. whitespace tokenizer\n",
        "#########################\n",
        "import re\n",
        "white_tok = text.split()\n",
        "print(white_tok)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['In', 'H.P.', \"Lovecraft's\", 'short', 'story', \"'The\", 'Call', 'of', \"Cthulhu',\", 'the', 'author', 'states', 'that', 'in', 'S.', 'Latitude', '47°', \"9',\", 'W.', 'Longitude', '126°', \"43'\", 'the', 'great', 'Cthulhu', 'dreams', 'in', 'the', 'sea-bottom', 'city', 'of', \"R'lyeh.\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meMNskYJVaeA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "06218313-8014-4c21-88b5-5d512854ec72"
      },
      "source": [
        "# 2. Natural Language Tool Kit tokenizer\n",
        "########################################\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "#nltk.download('punkt')\n",
        "\n",
        "tokens_NLTK = word_tokenize(text)\n",
        "print(tokens_NLTK)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['In', 'H.P', '.', 'Lovecraft', \"'s\", 'short', 'story', \"'The\", 'Call', 'of', 'Cthulhu', \"'\", ',', 'the', 'author', 'states', 'that', 'in', 'S.', 'Latitude', '47°', '9', \"'\", ',', 'W.', 'Longitude', '126°', '43', \"'\", 'the', 'great', 'Cthulhu', 'dreams', 'in', 'the', 'sea-bottom', 'city', 'of', \"R'lyeh\", '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAxwZcm_VaeM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "outputId": "23e928aa-0fc8-43c7-d9f7-7149bc69d490"
      },
      "source": [
        "# 3. SpaCy tokenizer\n",
        "####################\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In\n",
            "H.P.\n",
            "Lovecraft\n",
            "'s\n",
            "short\n",
            "story\n",
            "'\n",
            "The\n",
            "Call\n",
            "of\n",
            "Cthulhu\n",
            "'\n",
            ",\n",
            "the\n",
            "author\n",
            "states\n",
            "that\n",
            "in\n",
            "S.\n",
            "Latitude\n",
            "47\n",
            "°\n",
            "9\n",
            "'\n",
            ",\n",
            "W.\n",
            "Longitude\n",
            "126\n",
            "°\n",
            "43\n",
            "'\n",
            "the\n",
            "great\n",
            "Cthulhu\n",
            "dreams\n",
            "in\n",
            "the\n",
            "sea\n",
            "-\n",
            "bottom\n",
            "city\n",
            "of\n",
            "R'lyeh\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ5zLP9PVaeZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1b73c0a5-ff0b-4031-f9a5-6062c12bd387"
      },
      "source": [
        "# 4. sklearn vectorizer\n",
        "#######################\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "textt = [text]\n",
        "X = vectorizer.fit_transform(textt)\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['126', '43', '47', 'author', 'bottom', 'call', 'city', 'cthulhu', 'dreams', 'great', 'in', 'latitude', 'longitude', 'lovecraft', 'lyeh', 'of', 'sea', 'short', 'states', 'story', 'that', 'the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xrbEGuBVaem",
        "colab_type": "text"
      },
      "source": [
        "## 3.3. Stopwords removal<a name=\"stopwords\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwVevf4MVaer",
        "colab_type": "text"
      },
      "source": [
        "We now remove stopwords using NLTK, SpaCy and sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZTgN20WVaeu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "2cd1bfbc-aed2-48cc-9c1a-456f3f1f0c65"
      },
      "source": [
        "# 1. stopwords removal with the Natural Language Tool Kit (NLTK)\n",
        "################################################################\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# we tokenize the test sentence\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "filtered_tokens = [word for word in tokens if word not in stop]\n",
        "\n",
        "print('-----------------------------')\n",
        "print('NLTK tokenized test sentence:', tokens)\n",
        "print('-----------------------------')\n",
        "print('NLTK tokenized test sentence after stowords removal:', filtered_tokens)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "-----------------------------\n",
            "NLTK tokenized test sentence: ['In', 'H.P', '.', 'Lovecraft', \"'s\", 'short', 'story', \"'The\", 'Call', 'of', 'Cthulhu', \"'\", ',', 'the', 'author', 'states', 'that', 'in', 'S.', 'Latitude', '47°', '9', \"'\", ',', 'W.', 'Longitude', '126°', '43', \"'\", 'the', 'great', 'Cthulhu', 'dreams', 'in', 'the', 'sea-bottom', 'city', 'of', \"R'lyeh\", '.']\n",
            "-----------------------------\n",
            "NLTK tokenized test sentence after stowords removal: ['In', 'H.P', '.', 'Lovecraft', \"'s\", 'short', 'story', \"'The\", 'Call', 'Cthulhu', \"'\", ',', 'author', 'states', 'S.', 'Latitude', '47°', '9', \"'\", ',', 'W.', 'Longitude', '126°', '43', \"'\", 'great', 'Cthulhu', 'dreams', 'sea-bottom', 'city', \"R'lyeh\", '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L2xaYKGVae7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ea6d35a-61d9-4870-8223-4a5cc69db8c0"
      },
      "source": [
        "# removed stopwords\n",
        "list(set(tokens) - set(filtered_tokens))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['in', 'the', 'of', 'that']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhn8vhNQVafH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "bee01825-a703-4201-894a-454c81896870"
      },
      "source": [
        "# 2. stopwords removal with SpaCy\n",
        "#################################\n",
        "import spacy\n",
        "\n",
        "# tokenization\n",
        "text_spacy = nlp(text)\n",
        "\n",
        "token_list = []\n",
        "for token in text_spacy:\n",
        "    token_list.append(token.text)\n",
        "\n",
        "# stopwords\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# create list of word tokens after removing stopwords note that .vocab() looks at the lexeme of each token \n",
        "filtered_sentence =[] \n",
        "\n",
        "for word in token_list:\n",
        "    lexeme = nlp.vocab[word]   \n",
        "    if lexeme.is_stop == False:\n",
        "        filtered_sentence.append(word) \n",
        "        \n",
        "print('-----------------------------')\n",
        "print('SpaCy tokenized test sentence:', token_list)\n",
        "print('-----------------------------')\n",
        "print('SpaCy tokenized test sentence after stopwords removal:', filtered_sentence) "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------\n",
            "SpaCy tokenized test sentence: ['In', 'H.P.', 'Lovecraft', \"'s\", 'short', 'story', \"'\", 'The', 'Call', 'of', 'Cthulhu', \"'\", ',', 'the', 'author', 'states', 'that', 'in', 'S.', 'Latitude', '47', '°', '9', \"'\", ',', 'W.', 'Longitude', '126', '°', '43', \"'\", 'the', 'great', 'Cthulhu', 'dreams', 'in', 'the', 'sea', '-', 'bottom', 'city', 'of', \"R'lyeh\", '.']\n",
            "-----------------------------\n",
            "SpaCy tokenized test sentence after stowords removal: ['H.P.', 'Lovecraft', 'short', 'story', \"'\", 'Cthulhu', \"'\", ',', 'author', 'states', 'S.', 'Latitude', '47', '°', '9', \"'\", ',', 'W.', 'Longitude', '126', '°', '43', \"'\", 'great', 'Cthulhu', 'dreams', 'sea', '-', 'city', \"R'lyeh\", '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlMaPdsyVafV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1cf65fd4-d2df-48a0-bfe7-7e25e12076eb"
      },
      "source": [
        "# removed stopwords\n",
        "list(set(token_list) - set(filtered_sentence))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['in', 'the', 'In', 'The', 'that', 'Call', 'bottom', 'of', \"'s\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ-UMJxEVaff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "224f66ab-8df3-4318-b588-ada6d8d1c7da"
      },
      "source": [
        "# 3. stopwords removal with sklearn and TfidfVectorizer()\n",
        "########################################################\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [text]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer_stop = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "X_stop = vectorizer_stop.fit_transform(corpus)\n",
        "\n",
        "print('-----------------------------')\n",
        "print('CountVectorizer() tokenized test sentence:', vectorizer.get_feature_names())\n",
        "print('-----------------------------')\n",
        "print('CountVectorizer() tokenized test sentence after stopwords removal:', vectorizer_stop.get_feature_names()) "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------\n",
            "CountVectorizer() tokenized test sentence: ['126', '43', '47', 'author', 'bottom', 'call', 'city', 'cthulhu', 'dreams', 'great', 'in', 'latitude', 'longitude', 'lovecraft', 'lyeh', 'of', 'sea', 'short', 'states', 'story', 'that', 'the']\n",
            "-----------------------------\n",
            "CountVectorizer() tokenized test sentence after stowords removal: ['126', '43', '47', 'author', 'city', 'cthulhu', 'dreams', 'great', 'latitude', 'longitude', 'lovecraft', 'lyeh', 'sea', 'short', 'states', 'story']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlN2QGm2Vafs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20d3b29d-7611-46fd-9567-6a92c0c00800"
      },
      "source": [
        "# removed stopwords\n",
        "set(vectorizer.get_feature_names()) - set(vectorizer_stop.get_feature_names())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bottom', 'call', 'in', 'of', 'that', 'the'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x0B8mLkVaf2",
        "colab_type": "text"
      },
      "source": [
        "## 3.4. Part-of-speech tagging<a name=\"POS\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqz4qEbVVaf4",
        "colab_type": "text"
      },
      "source": [
        "We perform Part-Of-Speech (POS) tagging using the NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC-wmvbhVaf5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "0cc13311-f0c6-48ae-f5c0-4c327a8e66c0"
      },
      "source": [
        "# introduction of POS tagger per NLTK token (word_tokenize is the tokenizer we choose)\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "tokens_NLTK = word_tokenize(text)\n",
        "print(pos_tag(word_tokenize(text)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('In', 'IN'), ('H.P', 'NNP'), ('.', '.'), ('Lovecraft', 'NNP'), (\"'s\", 'POS'), ('short', 'JJ'), ('story', 'NN'), (\"'The\", 'POS'), ('Call', 'NNP'), ('of', 'IN'), ('Cthulhu', 'NNP'), (\"'\", 'POS'), (',', ','), ('the', 'DT'), ('author', 'NN'), ('states', 'VBZ'), ('that', 'IN'), ('in', 'IN'), ('S.', 'NNP'), ('Latitude', 'NNP'), ('47°', 'CD'), ('9', 'CD'), (\"'\", \"''\"), (',', ','), ('W.', 'NNP'), ('Longitude', 'NNP'), ('126°', 'CD'), ('43', 'CD'), (\"'\", \"''\"), ('the', 'DT'), ('great', 'JJ'), ('Cthulhu', 'NNP'), ('dreams', 'NN'), ('in', 'IN'), ('the', 'DT'), ('sea-bottom', 'JJ'), ('city', 'NN'), ('of', 'IN'), (\"R'lyeh\", 'NNP'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAaGmGwfVagV",
        "colab_type": "text"
      },
      "source": [
        "## 3.5. Stemming and lemmatization<a name=\"stemming\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRxTD34YVagY",
        "colab_type": "text"
      },
      "source": [
        "We perform (Porter) stemming and lemmatization on the test sentence, after tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbXfx5-jVagb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "39b5c699-f13b-4c3e-f0c9-40a09da960de"
      },
      "source": [
        "# NLTK Porter stemming on tokenized test sentence \n",
        "#################################################\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in word_tokenize(text)]\n",
        "\n",
        "# use stemming on NLTK tokenizer text\n",
        "stem_tokens = tokenizer_porter(text)\n",
        "print(stem_tokens)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['In', 'h.p', '.', 'lovecraft', \"'s\", 'short', 'stori', \"'the\", 'call', 'of', 'cthulhu', \"'\", ',', 'the', 'author', 'state', 'that', 'in', 'S.', 'latitud', '47°', '9', \"'\", ',', 'W.', 'longitud', '126°', '43', \"'\", 'the', 'great', 'cthulhu', 'dream', 'in', 'the', 'sea-bottom', 'citi', 'of', \"r'lyeh\", '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwBebEN-Vagl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "97327768-9da1-49ab-9910-95b7c0be4ce7"
      },
      "source": [
        "# NLTK lemmatization (WordNet database) on tokenized test sentence\n",
        "##################################################################\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Wordnet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# NLTK tokenizer\n",
        "word_list = nltk.word_tokenize(text)\n",
        "\n",
        "# lemmatization of the list of words and join - we lemmatize verbs (therefore 'v') and we use '***' as separator\n",
        "lemmatized_output = '***'.join([lemmatizer.lemmatize(w, 'v') for w in word_list])\n",
        "print(lemmatized_output)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "In***H.P***.***Lovecraft***'s***short***story***'The***Call***of***Cthulhu***'***,***the***author***state***that***in***S.***Latitude***47°***9***'***,***W.***Longitude***126°***43***'***the***great***Cthulhu***dream***in***the***sea-bottom***city***of***R'lyeh***.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}